import torch
from transformers import XLMTokenizer, RobertaModel

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Running on: {DEVICE}")


@torch.no_grad()
def get_embedding_for_text(text: str) -> (torch.tensor, torch.tensor):
    """
    For a given sentence the function return embedding generated by BERT
    :param text: Sentence for which u want to get an embedding
    :return: (tensor of embeddings for each token in sentnece, average embedding of a sentence)
    """
    tokenizer = XLMTokenizer.from_pretrained("allegro/herbert-klej-cased-tokenizer-v1")
    bert_model = RobertaModel.from_pretrained("allegro/herbert-klej-cased-v1")

    encoded_input = tokenizer.encode(text, return_tensors="pt").to(DEVICE)
    outputs = bert_model(encoded_input)

    sequence_tokens_embedding = outputs[0].squeeze(dim=0)
    sentence_embedding = outputs[1].squeeze(dim=0)
    return sequence_tokens_embedding, sentence_embedding
