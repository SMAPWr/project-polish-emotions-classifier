import torch
from transformers import XLMTokenizer, RobertaModel
from typing import List

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
print(f"Running on: {DEVICE}")


@torch.no_grad()
def get_embedding_for_text(text: str) -> (torch.tensor, torch.tensor):
    """
    For a given sentence the function return embedding generated by BERT
    :param text: Sentence for which u want to get an embedding
    :return: (tensor of embeddings for each token in sentnece, average embedding of a sentence)
    """
    tokenizer = XLMTokenizer.from_pretrained("allegro/herbert-klej-cased-tokenizer-v1")
    bert_model = RobertaModel.from_pretrained("allegro/herbert-klej-cased-v1")

    encoded_input = tokenizer.encode(text, return_tensors="pt").to(DEVICE)
    outputs = bert_model(encoded_input)

    sequence_tokens_embedding = outputs[0].squeeze(dim=0)
    sentence_embedding = outputs[1].squeeze(dim=0)
    return sequence_tokens_embedding, sentence_embedding


@torch.no_grad()
def get_embedding_for_list_of_texts(
    list_of_texts: List[str],
) -> (torch.tensor, torch.tensor):
    """
    For a given list of sentences the function return embedding generated by BERT
    :param list_of_texts:
    :param text: Sentence for which u want to get an embedding
    :return: (list of embeddings for each token in sentences, average embedding of a sentences of size (N, 768) where N
    is number of texts)
    """
    tokenizer = XLMTokenizer.from_pretrained("allegro/herbert-klej-cased-tokenizer-v1")
    bert_model = RobertaModel.from_pretrained("allegro/herbert-klej-cased-v1")

    list_of_sentence_embeddings = []
    list_of_sequence_embeddings = []

    for text in list_of_texts:
        encoded_input = tokenizer.encode(text, return_tensors="pt").to(DEVICE)
        outputs = bert_model(encoded_input)

        sequence_tokens_embedding = outputs[0].squeeze(dim=0)
        sentence_embedding = outputs[1].squeeze(dim=0)

        list_of_sequence_embeddings.append(sequence_tokens_embedding)
        list_of_sentence_embeddings.append(sentence_embedding)

    #     list_of_sequence_embeddings = torch.stack(list_of_sequence_embeddings, dim=0)
    list_of_sentence_embeddings = torch.stack(list_of_sentence_embeddings, dim=0)

    return list_of_sentence_embeddings, list_of_sequence_embeddings
